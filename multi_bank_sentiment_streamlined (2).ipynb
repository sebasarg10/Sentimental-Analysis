{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Multi-Bank Earnings Call Sentiment Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook compares sentiment across multiple banks' earnings calls using two complementary methods:\n",
    "\n",
    "**Dictionary Method:**\n",
    "- Counts positive and negative words from Loughran-McDonald financial dictionary\n",
    "- Score is a percentage: (Positive words - Negative words) / Total words × 100\n",
    "- Example: +2.5% means 2.5% more positive than negative words\n",
    "\n",
    "**FinBERT Method:**\n",
    "- AI model analyzes complete sentences and paragraphs\n",
    "- For each page, outputs probabilities: Positive, Negative, Neutral\n",
    "- Score is average net score: Average(Positive Probability - Negative Probability)\n",
    "- Example: +0.45 means average 45% more positive than negative probability\n",
    "\n",
    "### Why Scores Differ:\n",
    "\n",
    "The scales are fundamentally different:\n",
    "- Dictionary: Percentage of word differences (typical range: -3% to +3%)\n",
    "- FinBERT: Probability difference (range: -1.0 to +1.0, typical: -0.5 to +0.5)\n",
    "\n",
    "A Dictionary score of +2.0% and FinBERT score of +0.40 both indicate moderate positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config",
   "metadata": {},
   "source": [
    "## Step 1: Configure Banks to Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-banks",
   "metadata": {},
   "outputs": [],
   "source": [
    "banks_to_analyze = [\n",
    "    {\n",
    "        'name': 'Scotiabank',\n",
    "        'url': 'https://www.scotiabank.com/content/dam/scotiabank/corporate/quarterly-reports/2025/q1/BNS-T_Transcript_2025-02-25.pdf'\n",
    "    },\n",
    "    {\n",
    "        'name': 'RBC',\n",
    "        'url': 'https://www.rbc.com/investor-relations/_assets-custom/pdf/2025q1speech.pdf'\n",
    "    },\n",
    "    {\n",
    "        'name': 'TD Bank',\n",
    "        'url': 'https://www.td.com/content/dam/tdcom/canada/about-td/pdf/quarterly-results/2025/q1/2025-q1-td-transcript.pdf'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Configured {len(banks_to_analyze)} banks for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-all",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from pypdf import PdfReader\n",
    "import re\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-resources",
   "metadata": {},
   "source": [
    "## Step 3: Load Analysis Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-dict",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Loughran-McDonald dictionary\n",
    "lm_dict = pd.read_csv(\"Loughran-McDonald_MasterDictionary_1993-2024.csv\")\n",
    "lm_dict['word_lower'] = lm_dict['Word'].str.lower()\n",
    "positive_words_set = set(lm_dict[lm_dict['Positive'] != 0]['word_lower'].tolist())\n",
    "negative_words_set = set(lm_dict[lm_dict['Negative'] != 0]['word_lower'].tolist())\n",
    "\n",
    "print(f\"Loaded {len(positive_words_set):,} positive words\")\n",
    "print(f\"Loaded {len(negative_words_set):,} negative words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-stopwords",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords\n",
    "with open(\"stopwords.txt\", 'r') as f:\n",
    "    stopwords_set = set(word.strip().lower() for word in f.readlines())\n",
    "\n",
    "print(f\"Loaded {len(stopwords_set):,} stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-finbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FinBERT model\n",
    "print(\"Loading FinBERT model...\")\n",
    "finbert_tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "finbert_model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "print(\"FinBERT model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functions",
   "metadata": {},
   "source": [
    "## Step 4: Define Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_financial_text(text):\n",
    "    \"\"\"Normalize text: lowercase, remove punctuation, collapse whitespace\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, stopword_set):\n",
    "    \"\"\"Remove common words that don't carry sentiment\"\"\"\n",
    "    words = text.split()\n",
    "    filtered = [w for w in words if w not in stopword_set]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "def remove_boilerplate_text(text):\n",
    "    \"\"\"Remove page headers, footers, speaker names, and other boilerplate\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line_stripped = line.strip()\n",
    "        if not line_stripped:\n",
    "            continue\n",
    "        if re.search(r'Page\\s+\\d+\\s+of\\s+\\d+', line_stripped, re.IGNORECASE):\n",
    "            continue\n",
    "        if 'Earnings Call Transcript' in line_stripped or 'Transcript' in line_stripped:\n",
    "            continue\n",
    "        if len(line_stripped) < 60 and line_stripped.isupper():\n",
    "            if not any(w in line_stripped.lower() for w in ['the', 'and', 'our', 'we']):\n",
    "                continue\n",
    "        if (line_stripped.count(' - ') >= 2 or line_stripped.count('–') >= 2):\n",
    "            if len(line_stripped) < 100 and not line_stripped.endswith('.'):\n",
    "                continue\n",
    "        cleaned_lines.append(line_stripped)\n",
    "    \n",
    "    return ' '.join(cleaned_lines)\n",
    "\n",
    "def find_presentation_start(df):\n",
    "    \"\"\"Find the page where substantive content begins\"\"\"\n",
    "    for idx, row in df.iterrows():\n",
    "        text_upper = row['raw_text'].upper().replace(' ', '')\n",
    "        if 'PRESENTATION' in text_upper or 'MANAGEMENTDISCUSSION' in text_upper:\n",
    "            if 'PRESENTATION' in text_upper[:500] or 'MANAGEMENTDISCUSSION' in text_upper[:500]:\n",
    "                if row['page_number'] > 1:\n",
    "                    return row['page_number']\n",
    "    return 1\n",
    "\n",
    "def calculate_sentiment(text, pos_set, neg_set):\n",
    "    \"\"\"Dictionary method: Count positive and negative words\"\"\"\n",
    "    words = text.split()\n",
    "    pos_count = sum(1 for w in words if w in pos_set)\n",
    "    neg_count = sum(1 for w in words if w in neg_set)\n",
    "    net_sent = pos_count - neg_count\n",
    "    return pos_count, neg_count, net_sent\n",
    "\n",
    "def analyze_with_finbert(text, tokenizer, model):\n",
    "    \"\"\"FinBERT method: Analyze text and return probabilities\"\"\"\n",
    "    tokenized = tokenizer(text, return_tensors=\"pt\", truncation=True, \n",
    "                         max_length=512, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    pos_prob = probs[0][0].item()\n",
    "    neg_prob = probs[0][1].item()\n",
    "    neu_prob = probs[0][2].item()\n",
    "    \n",
    "    if pos_prob > neg_prob and pos_prob > neu_prob:\n",
    "        label = 'positive'\n",
    "    elif neg_prob > pos_prob and neg_prob > neu_prob:\n",
    "        label = 'negative'\n",
    "    else:\n",
    "        label = 'neutral'\n",
    "    \n",
    "    net_score = pos_prob - neg_prob\n",
    "    return pos_prob, neg_prob, neu_prob, label, net_score\n",
    "\n",
    "def analyze_sentence_finbert(sentence, tokenizer, model):\n",
    "    \"\"\"Analyze a single sentence with FinBERT\"\"\"\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    if len(sentence) < 20:\n",
    "        return None\n",
    "    if re.search(r'Page\\s+\\d+\\s+of\\s+\\d+', sentence, re.IGNORECASE):\n",
    "        return None\n",
    "    if 'Earnings Call Transcript' in sentence:\n",
    "        return None\n",
    "    if len(sentence) < 60 and sentence.count(' ') < 5:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        tokenized = tokenizer(sentence, return_tensors=\"pt\", truncation=True,\n",
    "                             max_length=512, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokenized)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        return {\n",
    "            'sentence': sentence[:300],\n",
    "            'positive_prob': probs[0][0].item(),\n",
    "            'negative_prob': probs[0][1].item(),\n",
    "            'net_score': probs[0][0].item() - probs[0][1].item()\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "print(\"Functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "process",
   "metadata": {},
   "source": [
    "## Step 5: Process All Banks\n",
    "\n",
    "This will download transcripts, extract text, and run both sentiment analysis methods.\n",
    "Processing happens silently with results stored for final comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-all-banks",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "all_bank_data = {}\n",
    "\n",
    "print(\"Processing banks...\\n\")\n",
    "\n",
    "for i, bank_config in enumerate(banks_to_analyze, 1):\n",
    "    bank_name = bank_config['name']\n",
    "    bank_url = bank_config['url']\n",
    "    \n",
    "    print(f\"{i}/{len(banks_to_analyze)}: {bank_name}\")\n",
    "    \n",
    "    # Download PDF\n",
    "    pdf_filename = f\"{bank_name.replace(' ', '_')}_transcript.pdf\"\n",
    "    response = requests.get(bank_url, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    Path(pdf_filename).write_bytes(response.content)\n",
    "    \n",
    "    # Extract text\n",
    "    reader = PdfReader(pdf_filename)\n",
    "    pages_data = []\n",
    "    for page_num, page in enumerate(reader.pages, start=1):\n",
    "        pages_data.append({\n",
    "            'page_number': page_num,\n",
    "            'raw_text': page.extract_text()\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(pages_data)\n",
    "    \n",
    "    # Find substantive content and clean\n",
    "    start_page = find_presentation_start(df)\n",
    "    df = df[df['page_number'] >= start_page].copy()\n",
    "    df['raw_text'] = df['raw_text'].apply(remove_boilerplate_text)\n",
    "    df['cleaned_text'] = df['raw_text'].apply(clean_financial_text)\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(lambda x: remove_stopwords(x, stopwords_set))\n",
    "    df['word_count'] = df['cleaned_text'].str.split().str.len()\n",
    "    \n",
    "    full_raw_text = ' '.join(df['raw_text'].tolist())\n",
    "    full_cleaned_text = ' '.join(df['cleaned_text'].tolist())\n",
    "    \n",
    "    # Dictionary analysis\n",
    "    sentiment_results = df['cleaned_text'].apply(\n",
    "        lambda text: calculate_sentiment(text, positive_words_set, negative_words_set)\n",
    "    )\n",
    "    df['dict_pos_count'] = sentiment_results.apply(lambda x: x[0])\n",
    "    df['dict_neg_count'] = sentiment_results.apply(lambda x: x[1])\n",
    "    df['dict_net_sent'] = sentiment_results.apply(lambda x: x[2])\n",
    "    df['dict_sent_ratio'] = ((df['dict_net_sent'] / df['word_count']) * 100).fillna(0)\n",
    "    \n",
    "    dict_total_pos = df['dict_pos_count'].sum()\n",
    "    dict_total_neg = df['dict_neg_count'].sum()\n",
    "    dict_total_words = df['word_count'].sum()\n",
    "    dict_overall_ratio = ((dict_total_pos - dict_total_neg) / dict_total_words) * 100\n",
    "    \n",
    "    # FinBERT analysis\n",
    "    finbert_results = []\n",
    "    for idx, row in df.iterrows():\n",
    "        pos, neg, neu, lbl, net = analyze_with_finbert(\n",
    "            row['raw_text'], finbert_tokenizer, finbert_model\n",
    "        )\n",
    "        finbert_results.append({\n",
    "            'page_number': row['page_number'],\n",
    "            'finbert_pos_prob': pos,\n",
    "            'finbert_neg_prob': neg,\n",
    "            'finbert_neu_prob': neu,\n",
    "            'finbert_label': lbl,\n",
    "            'finbert_net_score': net\n",
    "        })\n",
    "    \n",
    "    finbert_df = pd.DataFrame(finbert_results)\n",
    "    df = df.merge(finbert_df, on='page_number', how='left')\n",
    "    \n",
    "    finbert_avg_net = df['finbert_net_score'].mean()\n",
    "    finbert_label_counts = df['finbert_label'].value_counts()\n",
    "    \n",
    "    # Store results\n",
    "    all_results.append({\n",
    "        'bank': bank_name,\n",
    "        'dict_score': dict_overall_ratio,\n",
    "        'finbert_score': finbert_avg_net,\n",
    "        'dict_pos_words': dict_total_pos,\n",
    "        'dict_neg_words': dict_total_neg,\n",
    "        'finbert_pos_pages': finbert_label_counts.get('positive', 0),\n",
    "        'finbert_neg_pages': finbert_label_counts.get('negative', 0),\n",
    "        'total_pages': len(df)\n",
    "    })\n",
    "    \n",
    "    all_bank_data[bank_name] = {'df': df}\n",
    "\n",
    "print(\"\\nProcessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results",
   "metadata": {},
   "source": [
    "## Step 6: Results Summary\n",
    "\n",
    "### Understanding the Scores:\n",
    "\n",
    "**Dictionary Score (%):**\n",
    "- Range: Typically -3% to +3%\n",
    "- Interpretation: Percentage of net positive words out of total\n",
    "\n",
    "**FinBERT Score:**\n",
    "- Range: -1.0 to +1.0, typically -0.5 to +0.5\n",
    "- Interpretation: Average probability difference between positive and negative\n",
    "\n",
    "**Comparison Guidelines:**\n",
    "- Dictionary +2% ≈ FinBERT +0.40 (both moderately positive)\n",
    "- Focus on relative rankings between banks, not absolute differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame(all_results)\n",
    "comparison_df['dict_rank'] = comparison_df['dict_score'].rank(ascending=False).astype(int)\n",
    "comparison_df['finbert_rank'] = comparison_df['finbert_score'].rank(ascending=False).astype(int)\n",
    "\n",
    "# Display key metrics\n",
    "display_df = comparison_df[[\n",
    "    'bank', 'dict_score', 'dict_rank', 'finbert_score', 'finbert_rank'\n",
    "]].sort_values('finbert_score', ascending=False)\n",
    "\n",
    "print(\"\\nSentiment Rankings:\\n\")\n",
    "print(display_df.to_string(index=False))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-title",
   "metadata": {},
   "source": [
    "## Step 7: Comparative Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Dictionary comparison\n",
    "sorted_dict = comparison_df.sort_values('dict_score', ascending=False)\n",
    "x = range(len(sorted_dict))\n",
    "colors_dict = ['#06A77D' if s > 0 else '#E63946' for s in sorted_dict['dict_score']]\n",
    "\n",
    "bars1 = ax1.bar(x, sorted_dict['dict_score'], color=colors_dict,\n",
    "               edgecolor='black', linewidth=2, width=0.6)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(sorted_dict['bank'])\n",
    "ax1.axhline(y=0, color='black', linewidth=2)\n",
    "ax1.set_ylabel('Sentiment Ratio (%)', fontweight='bold')\n",
    "ax1.set_title('Dictionary Method: Bank Comparison', fontweight='bold', fontsize=13)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, score in zip(bars1, sorted_dict['dict_score']):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, height,\n",
    "            f'{score:+.2f}%', ha='center',\n",
    "            va='bottom' if height > 0 else 'top',\n",
    "            fontweight='bold', fontsize=11)\n",
    "\n",
    "# FinBERT comparison\n",
    "sorted_finbert = comparison_df.sort_values('finbert_score', ascending=False)\n",
    "x = range(len(sorted_finbert))\n",
    "colors_finbert = ['#06A77D' if s > 0 else '#E63946' for s in sorted_finbert['finbert_score']]\n",
    "\n",
    "bars2 = ax2.bar(x, sorted_finbert['finbert_score'], color=colors_finbert,\n",
    "               edgecolor='black', linewidth=2, width=0.6)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(sorted_finbert['bank'])\n",
    "ax2.axhline(y=0, color='black', linewidth=2)\n",
    "ax2.set_ylabel('Net Sentiment Score', fontweight='bold')\n",
    "ax2.set_title('FinBERT Method: Bank Comparison', fontweight='bold', fontsize=13)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, score in zip(bars2, sorted_finbert['finbert_score']):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, height,\n",
    "            f'{score:+.3f}', ha='center',\n",
    "            va='bottom' if height > 0 else 'top',\n",
    "            fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.suptitle('Multi-Bank Sentiment Comparison', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "examples-title",
   "metadata": {},
   "source": [
    "## Step 8: Example Sentences by Bank\n",
    "\n",
    "Top 3 most positive and most negative sentences identified by FinBERT for each bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bank_name in comparison_df.sort_values('finbert_score', ascending=False)['bank']:\n",
    "    print(f\"\\n{bank_name}\")\n",
    "    print()\n",
    "    \n",
    "    bank_df = all_bank_data[bank_name]['df']\n",
    "    \n",
    "    # Get top 3 positive pages\n",
    "    top_pos_pages = bank_df.nlargest(3, 'finbert_net_score')\n",
    "    \n",
    "    print(\"Top 3 Positive Sentences:\")\n",
    "    pos_count = 0\n",
    "    for _, page in top_pos_pages.iterrows():\n",
    "        for sent in page['raw_text'].split('.'):\n",
    "            result = analyze_sentence_finbert(sent, finbert_tokenizer, finbert_model)\n",
    "            if result and result['net_score'] > 0:\n",
    "                print(f\"  {pos_count + 1}. \\\"{result['sentence']}\\\"\")\n",
    "                print(f\"     Net: {result['net_score']:+.3f}\")\n",
    "                print()\n",
    "                pos_count += 1\n",
    "                if pos_count >= 3:\n",
    "                    break\n",
    "        if pos_count >= 3:\n",
    "            break\n",
    "    \n",
    "    # Get top 3 negative pages\n",
    "    top_neg_pages = bank_df.nsmallest(3, 'finbert_net_score')\n",
    "    \n",
    "    print(\"Top 3 Negative Sentences:\")\n",
    "    neg_count = 0\n",
    "    for _, page in top_neg_pages.iterrows():\n",
    "        for sent in page['raw_text'].split('.'):\n",
    "            result = analyze_sentence_finbert(sent, finbert_tokenizer, finbert_model)\n",
    "            if result and result['net_score'] < 0:\n",
    "                print(f\"  {neg_count + 1}. \\\"{result['sentence']}\\\"\")\n",
    "                print(f\"     Net: {result['net_score']:+.3f}\")\n",
    "                print()\n",
    "                neg_count += 1\n",
    "                if neg_count >= 3:\n",
    "                    break\n",
    "        if neg_count >= 3:\n",
    "            break\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-title",
   "metadata": {},
   "source": [
    "## Step 9: Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'multi_bank_sentiment_comparison.csv'\n",
    "comparison_df.to_csv(output_file, index=False)\n",
    "print(f\"Results exported to: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
